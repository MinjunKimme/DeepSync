
| Date       | Paper                                                                                        | Category               | Presenter | Link                                          |
| ---------- | -------------------------------------------------------------------------------------------- | ---------------------- | --------- | --------------------------------------------- |
| 2023.01.24 | Parameter-Efficient Transfer Learning for NLP                                                | PEFT                   | 나보영    | [Paper](https://arxiv.org/abs/1902.00751)     |
| 2023.01.31 | LoRA                                                                                         | PEFT                   | 강동규    | [Paper](https://arxiv.org/abs/2106.09685)</br> [Slide](https://github.com/devkade/DeepSync/tree/main/Docs/sprint3/LoRA.pdf)     |
| 2023.02.14 | Prefix-Tuning                                                                                | PEFT                   | 박지영    | [Paper](https://arxiv.org/abs/2101.00190)     |
|            | Few-Shot Parameter-Efficient Fine-Tuning vs. In-Context Learning                             | PEFT                   |           | [Paper](https://arxiv.org/abs/2205.05638)     |
|            | QLoRA                                                                                        | PEFT                   |           | [Paper](https://arxiv.org/abs/2305.14314)     |
|            | A Survey of Quantization Methods for Efficient Neural Network Inference                      | Quantization           |           | [Paper](https://arxiv.org/abs/2103.13630)     |
|            | Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference | Quantization           |           | [Paper](https://arxiv.org/abs/1712.05877)     |
| 2023.02.07 | Mixed Precision Training                                                                     | Quantization           | 이민지    | [Paper](https://arxiv.org/abs/1710.03740)     |
|            | Quant-Noise                                                                                  | Quantization           |           | [Paper](https://arxiv.org/abs/2004.07320)     |
|            | LLM.int8()                                                                                   | Quantization           |           | [Paper](https://arxiv.org/abs/2208.07339)     |
|            | GPTQ                                                                                         | Quantization           |           | [Paper](https://arxiv.org/abs/2210.17323)     |
| 2023.01.24 | Transformer 톺아보기                                                                         | Efficient Transformer  | 이승호    |                                               |
| 2023.02.07 | MegatronLM                                                                                   | Efficient Transformer  | 이승호    | [Paper](https://arxiv.org/abs/1909.08053)     |
| 2023.01.24 | Swin Transformer                                                                             | Efficient Transformer  | 오원준    | [Paper](https://arxiv.org/abs/2103.14030)     |
|            | zero                                                                                         | Efficient Transformer  |           | [Paper](https://arxiv.org/abs/1910.02054)     |
|            | zero infinity                                                                                | Efficient Transformer  |           | [Paper](https://arxiv.org/abs/2104.07857)     |
|            | Switch Transformer                                                                           | Efficient Transformer  |           | [Paper](https://arxiv.org/abs/2101.03961)     |
|            | DeepSpeed-MoE or Extreme Compression for Pre-trained Transformers Made Simple and Efficient  | Efficient Transformer  |           | [Paper](https://arxiv.org/abs/2201.05596)     |
| 2023.01.24 | Paged Attention                                                                              | Efficient Transformer  | 김민준    | [Paper](https://arxiv.org/abs/2309.06180)     |
|            | FlashAttention                                                                               | Efficient Transformer  |           | [Paper](https://arxiv.org/abs/2205.14135)     |
|            | FastViT                                                                                      | Efficient Transformer  |           | [Paper](https://arxiv.org/pdf/2303.14189.pdf) |                                             |
|            | Biformer                                                                                     | Efficient Transformer  |           | [Paper](https://arxiv.org/pdf/2303.08810.pdf) |
|            | Knowledge Distillation: A Survey                                                             | Knowledge Distillation |           | [Paper](https://arxiv.org/abs/2006.05525)     |
| 2023.02.07 | DeiT                                                                                         | Knowledge Distillation | 김해문    | [Paper](https://arxiv.org/abs/2012.12877)     |
| 2023.01.24 | Generative Agents: Interactive Simulacra of Human Behavior                                   | AI Agent               | 박지영    | [Paper](https://arxiv.org/abs/2304.03442)     |
|            | The Rise and Potential of Large Language Model Based Agents: A Survey                        | AI Agent               |           | [Paper](https://arxiv.org/abs/2309.07864)     |
|            | Agents: An Open-source Framework for Autonomous Language Agents                              | AI Agent               |           | [Paper](https://arxiv.org/abs/2309.07870)     |
|            | JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models      | AI Agent               |           | [Paper](https://arxiv.org/abs/2311.05997)     |
|            |                                                                                              |                        |           |                                               |
